{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast of Volatility using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elid-PC.DESKTOP-MG9DS93\\anaconda3\\lib\\site-packages\\pandas_datareader\\compat\\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, SimpleRNN\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "import sys, glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename for Summary Table: Summary_DJIA30.csv\n"
     ]
    }
   ],
   "source": [
    "#Summary Table\n",
    "csv = (glob.glob('*.csv'))[0]\n",
    "print ('Filename for Summary Table: ' + csv)\n",
    "\n",
    "#Data Preparation from CSV  \n",
    "df_Summary = pd.read_csv(csv,index_col=0,engine = 'python')\n",
    "n_days = 252 #Number of days to use as dataset based on average trading days per year \n",
    "\n",
    "#Define empty array\n",
    "dataset = np.empty((n_days, 0)) # locate the array\n",
    "\n",
    "#Create Dataset\n",
    "for i in range(0, len(df_Summary)):\n",
    "    try:\n",
    "        ticker = df_Summary['Ticker'][i]\n",
    "        df_ticker = pd.read_csv('Data//'+ ticker + '//' + ticker + '.csv',engine = 'python')\n",
    "\n",
    "        #Create new dataset array based on 'Return_log' and 'Volatility'\n",
    "        asset=df_ticker[['Ret_Log','Vol']].tail(n_days).values\n",
    "        dataset = np.append(dataset, asset, axis=1)\n",
    "    \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The dataset are then further divided by a factor of 2 to get a min of 90% of samples within the +/- 1 range. This is used for hyperbolic tangent (tanh) function for LSTM. This will prevent saturation and non-convergence of the model. Transform dataset into supervised learning model. Define function by taking the dataset and the look-back of 20 days as arguments and returns the input and target data based on LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing the LSTM model\n",
    "\n",
    "# Normalize data\n",
    "factor = 2\n",
    "\n",
    "# Calculate second raw moment\n",
    "M2 = np.mean(dataset ** 2, axis=0) ** (1/2)\n",
    "\n",
    "# Apply scaling\n",
    "dataset_norm = (1/factor) * (dataset / M2)\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    \"\"\"\n",
    "    Function to convert series from dataset to supervised learning problem\n",
    "    \"\"\"\n",
    "    data_x, data_y = [], []\n",
    "\n",
    "    for i in range(len(dataset) - look_back):\n",
    "\n",
    "        # Create sequence of length equal to look_back\n",
    "        x = dataset[i:(i + look_back), :]\n",
    "        data_x.append(x)\n",
    "\n",
    "        # Take just the volatility for the target\n",
    "        data_y.append(dataset[i + look_back, 1::2])\n",
    "\n",
    "    return np.array(data_x), np.array(data_y)\n",
    "\n",
    "# Convert series to supervised learning model\n",
    "look_back = 20\n",
    "X, y = create_dataset(dataset_norm, look_back)\n",
    "\n",
    "# Declare variables\n",
    "n_features = dataset.shape[1]\n",
    "n_assets = y.shape[1]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split the dataset set into training and test. Initialise 90 training days to fit the model then evuluate the trained data for the remaining days. After that, compile into LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (20,0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-54dbc9fdf05a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Prepare the 3D input vector for the LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    290\u001b[0m            [5, 6]])\n\u001b[0;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m--> 292\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (20,0)"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "training_days = 90\n",
    "X_train, X_test = X[:training_days], X[training_days:]\n",
    "y_train, y_test = y[:training_days], y[training_days:]\n",
    "\n",
    "# Prepare the 3D input vector for the LSTM\n",
    "X_train = np.reshape(X_train, (-1, look_back, n_features))\n",
    "X_test = np.reshape(X_test, (-1, look_back, n_features))\n",
    "\n",
    "# Interested on 1 step ahead of preductions on the testing set\n",
    "batch_size = 1\n",
    "\n",
    "# Create a sequential model in order to add layers to the model\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM considers the arguments the number of units and the input shape. \n",
    "# Regularisation is to deal with the undesired overfitting\n",
    "model.add(LSTM(len(dataset[0]),\n",
    "               input_shape=(look_back, n_features),\n",
    "               batch_size=batch_size,\n",
    "               stateful=True,\n",
    "               activity_regularizer=regularizers.l1_l2(),\n",
    "               recurrent_regularizer=regularizers.l1_l2()))\n",
    "\n",
    "# Setting dropout rate as 0 during training\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Dense layer are the number of unit and non-linear function based on sigmoid\n",
    "model.add(Dense(n_assets, activation='sigmoid')) \n",
    "\n",
    "\n",
    "# Compile the LSTM model\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following illustrates the training of the LSTM model. Proposed 20 days for fitting of the data then followed by prediction on the test dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# Fit the model\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              batch_size=batch_size,\n",
    "              shuffle=False,\n",
    "              epochs=1,\n",
    "              verbose=0) # Verbosity mode 0: silent\n",
    "\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction (rolling test window)\n",
    "y_pred = np.empty((0, n_assets))\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_i = X_test[i].reshape(1, look_back, n_features)\n",
    "    predicted_output = model.predict(X_i, batch_size=batch_size)\n",
    "\n",
    "    # Reshape prediction to save into array\n",
    "    predicted_output = predicted_output.reshape(1, n_assets)\n",
    "    y_pred = np.append(y_pred, predicted_output, axis=0)\n",
    "\n",
    "y_pred = y_pred.reshape(-1, n_assets)\n",
    "y_true = y_test.reshape(-1, n_assets)\n",
    "\n",
    "# Invert scaling\n",
    "def invert_standardization(data, M2, factor):\n",
    "  \n",
    "    # Consider just volatility series\n",
    "    M2 = M2[1::2]\n",
    "\n",
    "    data = factor * data * M2\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply inversion\n",
    "y_pred = invert_standardization(y_pred, M2, factor)\n",
    "y_true = invert_standardization(y_true, M2, factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate MSE and QLIKE\n",
    "    \"\"\"\n",
    "\n",
    "    mse = []\n",
    "    qlike = []\n",
    "\n",
    "    for i in range(0, int(len(dataset[0])/2)):\n",
    "        ticker = df_Summary['Ticker'][i]\n",
    "        mse_i = (y_true[:, i] - y_pred[:, i]) ** 2\n",
    "        qlike_i = np.log(y_pred[:, i]) + (y_true[:, i] /  y_pred[:, i])\n",
    "       \n",
    "        # save results (point by point)\n",
    "        results = np.array([mse_i, qlike_i]).transpose()\n",
    "        np.savetxt('Data//' + ticker + '//' + ticker + '_LTSM.csv', results, \n",
    "                   delimiter=',', header='MSE, Q-LIKE', fmt='%10.5f', comments='')\n",
    "        \n",
    "        mse.append(np.mean(mse_i, axis=0))\n",
    "        qlike.append(np.mean(qlike_i, axis=0))\n",
    "\n",
    "    return mse, qlike\n",
    "\n",
    "# Apply EVALUATE function to predictions\n",
    "mse, qlike = evaluate(y_true, y_pred)\n",
    "\n",
    "# save results\n",
    "results = np.array([mse, qlike]).transpose()\n",
    "\n",
    "df = pd.DataFrame({'MSE_LTSM': mse, 'QLIKE_LTSM': qlike})\n",
    "print(df.describe())\n",
    "\n",
    "df_Summary['MSE_LTSM'] = df['MSE_LTSM'] \n",
    "df_Summary['QLIKE_LTSM'] = df['QLIKE_LTSM'] \n",
    "df_Summary.to_csv(csv,encoding='utf-8', index=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
